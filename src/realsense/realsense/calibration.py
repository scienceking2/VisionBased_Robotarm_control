import cv2
import pyrealsense2 as rs
import numpy as np
import math

# ==========================================
# [ì‚¬ìš©ì ì„¤ì • ì˜ì—­] ì´ ë¶€ë¶„ì„ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”!
# ==========================================

# 1. ì•„ë£¨ì½” ë§ˆì»¤ì˜ í•œ ë³€ì˜ ê¸¸ì´ (ë‹¨ìœ„: ë¯¸í„°)
MARKER_SIZE = 0.096  # ì˜ˆ: 10cmë¼ë©´ 0.1

# 2. ë¡œë´‡ìœ¼ë¡œ ì¸¡ì •í•œ ë§ˆì»¤ ì •ì¤‘ì•™ì˜ ì¢Œí‘œ (ë‹¨ìœ„: ë¯¸í„°)
# ë¡œë´‡ TCPë¥¼ ë§ˆì»¤ ì¤‘ì•™ì— ì°ì—ˆì„ ë•Œì˜ ì¢Œí‘œ (X, Y, Z)
MARKER_IN_ROBOT_FRAME = np.array([0.271, 0.0, 0.130]) # ì˜ˆì‹œê°’: X=450mm ì§€ì 

# 3. ë§ˆì»¤ì˜ ë°©í–¥ ë³´ì • (ë¡œë´‡ ì¢Œí‘œê³„ ê¸°ì¤€)
# ë§ˆì»¤ë¥¼ ë¡œë´‡ X, Yì¶•ê³¼ í‰í–‰í•˜ê²Œ ë¶™ì˜€ë‹¤ë©´ íšŒì „ì€ Identity í–‰ë ¬ì— ê°€ê¹ìŠµë‹ˆë‹¤.
# ë§Œì•½ ë§ˆì»¤ê°€ ëŒì•„ê°€ ìˆë‹¤ë©´ ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í•˜ì§€ë§Œ, ì¼ë‹¨ í‰í–‰í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# (í•„ìš”ì‹œ Zì¶• íšŒì „ ë³€í™˜ ì¶”ê°€ ê°€ëŠ¥)
ROBOT_TO_MARKER_ROTATION = np.eye(3) 

# ==========================================

def get_transform_matrix(rvec, tvec):
    """ rvec, tvecë¥¼ 4x4 ë³€í™˜ í–‰ë ¬ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
    mat = np.eye(4)
    R, _ = cv2.Rodrigues(rvec)
    mat[:3, :3] = R
    mat[:3, 3] = tvec.flatten()
    return mat

def main():
    # 1. RealSense ì´ˆê¸°í™”
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
    profile = pipeline.start(config)

    # ë‚´ì¥ íŒŒë¼ë¯¸í„°(Intrinsics) ê°€ì ¸ì˜¤ê¸°
    depth_sensor = profile.get_device().first_depth_sensor()
    depth_scale = depth_sensor.get_depth_scale()
    
    intrinsics = profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()
    cam_matrix = np.array([
        [intrinsics.fx, 0, intrinsics.ppx],
        [0, intrinsics.fy, intrinsics.ppy],
        [0, 0, 1]
    ])
    dist_coeffs = np.array(intrinsics.coeffs)

    # 2. ArUco ì„¤ì • (DICT_5X5_250 ë“± ë³¸ì¸ì´ ì“°ëŠ” ë§ˆì»¤ ì¢…ë¥˜ë¡œ ë³€ê²½ í•„ìš”)
    # ì‚¬ì§„ ì† ë§ˆì»¤ëŠ” 5x5 ë˜ëŠ” 4x4ë¡œ ë³´ì…ë‹ˆë‹¤. (ì¼ë‹¨ 5x5ë¡œ ì‹œë„)
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    parameters = cv2.aruco.DetectorParameters()

    # ê³„ì‚°ëœ ìµœì¢… ë³€í™˜ í–‰ë ¬ì„ ì €ì¥í•  ë³€ìˆ˜
    T_base_to_camera = None

    print("--- ArUco ë§ˆì»¤ë¥¼ ì°¾ì•„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    print("ë§ˆì»¤ê°€ ë³´ì´ë©´ ìë™ìœ¼ë¡œ ë³€í™˜ í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")

    try:
        while True:
            frames = pipeline.wait_for_frames()
            depth_frame = frames.get_depth_frame()
            color_frame = frames.get_color_frame()
            if not depth_frame or not color_frame:
                continue

            # í”„ë ˆì„ ì •ë ¬ (Depth -> Color)
            align = rs.align(rs.stream.color)
            frames = align.process(frames)
            aligned_depth_frame = frames.get_depth_frame()
            
            color_image = np.asanyarray(color_frame.get_data())
            depth_image = np.asanyarray(aligned_depth_frame.get_data())

            # ArUco ë§ˆì»¤ ê²€ì¶œ
            gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
            corners, ids, rejected = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=parameters)

            if ids is not None and T_base_to_camera is None:
                # ë§ˆì»¤ê°€ ë°œê²¬ë˜ì—ˆê³ , ì•„ì§ í–‰ë ¬ ê³„ì‚° ì „ì´ë¼ë©´
                rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners, MARKER_SIZE, cam_matrix, dist_coeffs)
                
                # ì²« ë²ˆì§¸ ë°œê²¬ëœ ë§ˆì»¤(ì¸ë±ìŠ¤ 0)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìŒ
                rvec = rvecs[0]
                tvec = tvecs[0]

                # ì‹œê°í™” (ì¶• ê·¸ë¦¬ê¸°)
                cv2.drawFrameAxes(color_image, cam_matrix, dist_coeffs, rvec, tvec, 0.1)
                
                # ---------------------------------------------------------
                # [í•µì‹¬] ì¢Œí‘œ ë³€í™˜ í–‰ë ¬ ê³„ì‚° (ë™ì°¨ ë³€í™˜)
                # ---------------------------------------------------------
                
                # 1. T_camera_to_marker (ì¹´ë©”ë¼ ê¸°ì¤€ ë§ˆì»¤ì˜ ìœ„ì¹˜)
                T_cam_marker = get_transform_matrix(rvec, tvec)
                
                # 2. T_base_to_marker (ë¡œë´‡ ë² ì´ìŠ¤ ê¸°ì¤€ ë§ˆì»¤ì˜ ìœ„ì¹˜ - ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°’)
                T_base_marker = np.eye(4)
                T_base_marker[:3, :3] = ROBOT_TO_MARKER_ROTATION
                T_base_marker[:3, 3] = MARKER_IN_ROBOT_FRAME
                
                # 3. T_base_to_camera (ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ ê²ƒ: ë¡œë´‡ ë² ì´ìŠ¤ ê¸°ì¤€ ì¹´ë©”ë¼ì˜ ìœ„ì¹˜)
                # ìˆ˜ì‹: T_base_cam = T_base_marker * inv(T_cam_marker)
                T_base_to_camera = np.dot(T_base_marker, np.linalg.inv(T_cam_marker))

                print("\nâœ… ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
                print("--- ê³„ì‚°ëœ T_base_to_camera í–‰ë ¬ ---")
                print(np.array_str(T_base_to_camera, precision=3, suppress_small=True))
                print("------------------------------------\n")

            # ë§ˆì»¤ê°€ ê°ì§€ë˜ë©´ í…Œë‘ë¦¬ ê·¸ë¦¬ê¸°
            if ids is not None:
                cv2.aruco.drawDetectedMarkers(color_image, corners, ids)

            # ë§ˆìš°ìŠ¤ í´ë¦­ ì´ë²¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
            def mouse_callback(event, x, y, flags, param):
                if event == cv2.EVENT_LBUTTONDOWN:
                    if T_base_to_camera is None:
                        print("âš ï¸ ì•„ì§ ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í•´ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ ì•ˆ ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        return
                    
                    # 1. í”½ì…€(u,v) -> ì¹´ë©”ë¼ ì¢Œí‘œê³„(Xc, Yc, Zc) ë³€í™˜
                    depth_val = aligned_depth_frame.get_distance(x, y) # ë¯¸í„° ë‹¨ìœ„
                    if depth_val <= 0:
                        print("âš ï¸ ê¹Šì´ ê°’ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return

                    point_camera = rs.rs2_deproject_pixel_to_point(intrinsics, [x, y], depth_val)
                    
                    # ë™ì°¨ ì¢Œí‘œë¡œ ë³€í™˜ [x, y, z, 1]
                    point_camera_homo = np.array([point_camera[0], point_camera[1], point_camera[2], 1.0])

                    # 2. ì¹´ë©”ë¼ ì¢Œí‘œê³„ -> ë¡œë´‡ ì¢Œí‘œê³„ ë³€í™˜ (í–‰ë ¬ ê³±ì…ˆ)
                    point_robot_homo = np.dot(T_base_to_camera, point_camera_homo)

                    final_x = point_robot_homo[0] * 1000 # mm ë³€í™˜
                    final_y = point_robot_homo[1] * 1000
                    final_z = point_robot_homo[2] * 1000

                    print(f"í´ë¦­ ì¢Œí‘œ(í”½ì…€): ({x}, {y}) / Depth: {depth_val:.3f}m")
                    print(f"ğŸ¯ ë¡œë´‡ ëª©í‘œ ì¢Œí‘œ: X={final_x:.1f}, Y={final_y:.1f}, Z={final_z:.1f}")
                    print("--------------------------------------------------")

            cv2.namedWindow('RealSense ArUco', cv2.WINDOW_AUTOSIZE)
            cv2.setMouseCallback('RealSense ArUco', mouse_callback)
            cv2.imshow('RealSense ArUco', color_image)

            key = cv2.waitKey(1)
            if key & 0xFF == ord('q') or key == 27:
                break

    finally:
        pipeline.stop()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()